apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: example-workload-low-cpu-${UUID}
  namespace: default
  labels:
    app: spark-job-${UUID}
    applicationId: example-workload-low-cpu-${UUID}
    queue: root.default
spec:
  sparkConf:
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.hadoop.fs.s3a.aws.credentials.provider": "com.amazonaws.auth.InstanceProfileCredentialsProvider"
    "spark.kubernetes.local.dirs.tmpfs": "true"
    "spark.io.encryption.enabled": "true"
    "spark.eventLog.enabled": "true"
    "spark.eventLog.dir": "s3a://${AWS_S3_BUCKET_SPARK_UI}/spark-ui"
    "‚Äçspark.dynamicAllocation.enabled": "true"
    "spark.dynamicAllocation.shuffleTracking.enabled": "true"
    "spark.dynamicAllocation.shuffleTracking.timeout": "120"
    "spark.dynamicAllocation.minExecutors": "1"
    "spark.dynamicAllocation.maxExecutors": "2"
    "spark.kubernetes.allocation.batch.size": "15"
    "spark.dynamicAllocation.executorAllocationRatio": "1"
    "spark.dynamicAllocation.schedulerBacklogTimeout": "1"
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "${AWS_ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com/spark-custom:3.2.0"
  imagePullPolicy: IfNotPresent
  mainApplicationFile: "s3a://${AWS_S3_BUCKET_SPARK_UI}/data/src/jobs/example-001-csv2parquet.py"
  arguments:
    [
      "s3a://${AWS_S3_BUCKET_SPARK_UI}/data/files/dataset-example.csv",
      "s3a://${AWS_S3_BUCKET_SPARK_UI}/output/dataset-example/",
    ]
  sparkVersion: "3.2.0"
  restartPolicy:
    type: Never
  volumes:
    - name: "spark-volume-testing-${UUID}"
      hostPath:
        path: "/tmp"
        type: Directory
  timeToLiveSeconds: 5
  batchScheduler: yunikorn
  driver:
    cores: 1
    coreRequest: "850m"
    memory: "2450m"
    labels:
      version: 3.2.0
    serviceAccount: spark
    volumeMounts:
      - name: "spark-volume-testing-${UUID}"
        mountPath: "/tmp"
    annotations:
      yunikorn.apache.org/schedulingPolicyParameters: "placeholderTimeoutSeconds=30"
      yunikorn.apache.org/task-group-name: "spark-driver-${UUID}"
      yunikorn.apache.org/task-groups: |-
        [{
          "name": "spark-driver-${UUID}",
          "minMember": 1,
          "minResource": {
            "cpu": "850m",
            "memory": "2450m"
          },
          "nodeSelector": {
            "workload": "${TYPE_WORKLOAD}-driver"
          }
        },
        {
          "name": "spark-executor-${UUID}",
          "minMember": 2,
          "minResource": {
            "cpu": "850m",
            "memory": "2400m"
          },
          "nodeSelector": {
            "workload": "${TYPE_WORKLOAD}-executor"
          }
        }]
  executor:
    cores: 1
    instances: 2
    coreRequest: "850m"
    memory: "2400m"
    labels:
      version: 3.2.0
    volumeMounts:
      - name: "spark-volume-testing-${UUID}"
        mountPath: "/tmp"
    annotations:
      yunikorn.apache.org/task-group-name: "spark-executor-${UUID}"
