apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: example-001-${UUID}
  namespace: default
spec:
  sparkConf:
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.kubernetes.allocation.batch.size": "15"
    "spark.kubernetes.local.dirs.tmpfs": "true"
    "spark.io.encryption.enabled": "true"
    "spark.eventLog.enabled": "true"
    "spark.eventLog.dir": "s3a://${AWS_S3_BUCKET_SPARK_UI}/spark-ui"
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "${AWS_ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com/spark-custom:3.0.3"
  imagePullPolicy: Always
  mainApplicationFile: "s3a://${AWS_S3_BUCKET_SPARK_UI}/data/src/jobs/example-001-csv2parquet.py"
  arguments: [
    "s3a://${AWS_S3_BUCKET_SPARK_UI}/data/files/dataset-example.csv",
    "s3a://${AWS_S3_BUCKET_SPARK_UI}/output/dataset-example/"
  ]
  sparkVersion: "3.0.3"
  restartPolicy:
    type: Never
  volumes:
    - name: "spark-volume-testing"
      hostPath:
        path: "/tmp"
        type: Directory
  nodeSelector:
    intent: "${TYPE_WORKLOAD}"
  driver:
    cores: 1
    coreLimit: "1000m"
    memory: "512m"
    labels:
      version: 3.0.3
    serviceAccount: spark
    volumeMounts:
      - name: "spark-volume-testing"
        mountPath: "/tmp"
  executor:
    cores: 1
    instances: 1
    memory: "1000m"
    labels:
      version: 3.0.3
    volumeMounts:
      - name: "spark-volume-testing"
        mountPath: "/tmp"
